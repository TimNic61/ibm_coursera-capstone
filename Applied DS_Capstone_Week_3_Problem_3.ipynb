{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Segmenting and Clustering Neighborhoods of Toronto", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "### 1. Fetching data to define neighborhood", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "To be able to cluster different neighborhoods of the city of Toronto, we will need to define their geospatial locations and boundaries.  We can obtain this data from `geopy`.  But, before we can do that we have to know more about each neigborhood like thier names and postal codes.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "We will be able to achieve this by scraping the necessary data from a website.  the information we need is available at `https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M`", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#!conda install -c conda-forge beautifulsoup4  #remove leading hashtag if beutifulsoup is not installed.\n\nfrom bs4 import BeautifulSoup # the beautiful soup library will be used to scrape the data from wikipedia.\n\n#!conda install -c conda-forge lxml # remove leading hashtag if lxml parser is not installed\n\n#!conda install -c conda-forge requests # remove leading hashtag if requests library is not installed\nimport requests\nimport csv\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#The contents of the webpage are fetched and stored in as the variable 'source'.  'source' is passed into beautiful soup and parsed to return the beautiful soup object.\n\nsource = requests.get('https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M').text\nsoup = BeautifulSoup(source, 'lxml')\n\n# To work in the portion of the parse tree that we are concerned with, the contents of the table containing the data of interest is stored in the variable 'table'.\ntable = soup.table.tbody \n\n# A csv file is created so that the table contents can be written to it.\ncsv_file = open('toronto_hood.csv', 'w')\ncsv_writer = csv.writer(csv_file)\n\n# Each row of the table can be looped through and written to the csv file 'toronto_hood.csv'.\nfor table_row in table.find_all('tr'):\n    field_one = table_row.next_element.next_element\n    column_one = field_one.text\n    \n    field_two = field_one.next_sibling.next_sibling\n    column_two = field_two.text\n\n    # The /n linending must be removed from the neighbourhood and the 'Not assigned' values be changed to NaN so that pandas can recognize them.\n    field_three = field_two.next_sibling.next_sibling\n    column_three = field_three.text\n    column_three = column_three[:-1]\n    \n    if column_three == \"Not assigned\":\n        column_three = 'NaN'\n\n    table_row = table_row.next_sibling.next_sibling\n    \n    csv_writer.writerow([column_one, column_two, column_three])\n\ncsv_file.close()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# import libraries\n\nimport pandas as pd"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The dataframe is created from the csv.\ndf = pd.read_csv('toronto_hood.csv')\n\ndf.head()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The rows containing NaN are recognized by pandas and can be dropped and the index reset.\n\ndf.dropna(axis=0, inplace=True)\ndf.reset_index(inplace=True, drop=True)\ndf.head()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "_kg_hide-output": false
            }, 
            "outputs": [], 
            "source": "# The neighbourhoods are grouped and concantenated into a pandas Series for each postcode and assigned to a variable.\nbuild_list = df.groupby('Postcode')['Neighbourhood'].apply(', '.join)\n\n# A vector containing the distinct postcodes is created so that it can be looped through so the new values for 'Neighbourhood' in 'df'\npc_frame = df['Postcode'].unique()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# By iterating through the the column of Postcodes in 'pc_frame' the concantenated neighbourhoods can be written into the 'Neighbourhood' column of the original dataframe.\n\nfor i in range(102):\n    n_hood = pc_frame[i]\n    df['Neighbourhood'].loc[df['Postcode']== n_hood] = build_list[n_hood]\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# As each postcode had one row per each neighbourhood belonging to it, many duplicate rows were created in the for-loop above.  These duplicates are dropped to obtain the final dataframe.\ndf.drop_duplicates(inplace=True)\ndf.reset_index(inplace=True, drop=True)\ndf.shape\n\ndf.tail()"
        }, 
        {
            "source": "### 2. Adding longitude and latitude data to dataframe", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "There was an issue in trying to obtain the geocoder library from Anaconda.  However, the geospatial data for Toronto is available from another source.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "path = 'https://cocl.us/Geospatial_data'\n\nlat_long_df = pd.read_csv(path, index_col=False)\nlat_long_df.rename(columns={'Postal Code': 'Postcode'}, inplace=True)\nlat_long_df.head()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "toronto_data = df.merge(lat_long_df)\n\ntoronto_data.head()"
        }, 
        {
            "source": "### 3. Map of Toronto with Neighbourhoods", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "To dive into the data deeper, additional libraris are necessary for data manipulation and plotting.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import seaborn as sns\nimport folium\nimport numpy as np\n\nimport matplotlib.cm as cm\nimport matplotlib.colors as colors\n\nfrom pandas.io.json import json_normalize"
        }, 
        {
            "source": "To begin, the map of Toronto is created.  As a starting point, the Downtown area will serve as the center of the map.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "start_lat = toronto_data.at[2,'Latitude']\nstart_long = toronto_data.at[2, 'Longitude']\n\nmap_toronto = folium.Map(location=[start_lat, start_long], zoom_start=11)\n\n# add the markers for each postcode\nfor lat, lng, borough, neighborhood in zip(toronto_data['Latitude'], toronto_data['Longitude'], toronto_data['Borough'], toronto_data['Neighbourhood']):\n    label = '{}, {}'.format(neighborhood, borough)\n    label = folium.Popup(label, parse_html=True)\n    folium.CircleMarker(\n        [lat, lng],\n        radius=5,\n        popup=label,\n        color='blue',\n        fill=True,\n        fill_color='#3186cc',\n        fill_opacity=0.7,\n        parse_html=False).add_to(map_toronto)  \n    \nmap_toronto"
        }, 
        {
            "source": "With the map ready, it's now time to call the Foursquare API.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "source": "The resulting json is parsed for the venues by each neighborhood group.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def getNearbyVenues(names, latitudes, longitudes, radius=500):\n    \n    venues_list=[]\n    for name, lat, lng in zip(names, latitudes, longitudes):\n        print(name)\n            \n        # create the API request URL\n        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n            CLIENT_ID, \n            CLIENT_SECRET, \n            VERSION, \n            lat, \n            lng, \n            radius, \n            LIMIT)\n            \n        # make the GET request\n        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n        \n        # return only relevant information for each nearby venue\n        venues_list.append([(\n            name, \n            lat, \n            lng, \n            v['venue']['name'], \n            v['venue']['location']['lat'], \n            v['venue']['location']['lng'],  \n            v['venue']['categories'][0]['name']) for v in results])\n\n    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n    nearby_venues.columns = ['Neighborhood', \n                  'Neighborhood Latitude', \n                  'Neighborhood Longitude', \n                  'Venue', \n                  'Venue Latitude', \n                  'Venue Longitude', \n                  'Venue Category']\n    \n    return(nearby_venues)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "toronto_venues = getNearbyVenues(names=toronto_data['Neighbourhood'],\n                                   latitudes=toronto_data['Latitude'],\n                                   longitudes=toronto_data['Longitude']\n                                  )"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "venue_count = toronto_venues.shape[0]\n\nint(venue_count)\ntoronto_venues.head()\n\nprint(venue_count)"
        }, 
        {
            "source": "Our resulting data contains the top 120 venues found within 500 meters of the center of each of the 102 areas by postcode.  This could mean that neighbourhoods in larger postocodes may not in fact have any representation within the data.  Without neighbourhood-specific longitude and latitude data, this cannot be explored or confirmed.  The opposite of this \"scarcity\" issue may exist as well, where areas with higher density of both population and venues are likely to be defined in \"smaller\" postcodes.  The situation may arise where a venue may be within 500 meters of more than one geographic center of a postcode.  This can be evaluated by checking for duplicate venues within the data. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "toronto_venues.duplicated().value_counts()"
        }, 
        {
            "source": "The value counts returns 'False' for all venues from the data.  This means that in fact no venues were duplicated in the data fetched from the source.  In an attempt to force duplicate venues to occur, the data was fetched gain but with LIMIT increased from 120 to 250 and the radius increased from 500 to 800.  The result consisted of the same number of venues.  This would suggest that all venues in Toronto have been captured.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "toronto_venues.groupby('Neighborhood').count()"
        }, 
        {
            "source": "Seeing as we are healthy eaters, we are not interested in fast food restaurants.  These will be removed so as to not impact the clustering results.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "for i in range(venue_count):\n    if toronto_venues.loc[i,'Venue Category'] == 'Fast Food Restaurant':\n        toronto_venues.drop([i], inplace = True)\ntoronto_venues.reset_index(inplace=True, drop=True)\n        \ntoronto_venues.head()\n\n\n\n\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print(toronto_venues.shape)"
        }, 
        {
            "source": "", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# one hot encoding\n# the line below considers the venue categories in the new dataframe.  The neighborhoods are dropped and will need to be added back in.\ntoronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n\n# add neighborhood column back to dataframe\ntoronto_onehot['Neighborhood'] = toronto_venues['Neighborhood']\n# print(toronto_onehot.columns[0])\n\n# The line above added the neighborhood column to the end of the frame.  This can be moved back to the first column...\nwhile toronto_onehot.columns[0] != 'Neighborhood':\n    fixed_columns = [toronto_onehot.columns[-1]] + list(toronto_onehot.columns[:-1])\n    toronto_onehot = toronto_onehot[fixed_columns]\n    if toronto_onehot.columns[0] == 'Afghan Restaurant':\n        break\n\ntoronto_onehot.head(12)\n# print(toronto_onehot.columns)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "toronto_grouped = toronto_onehot.groupby('Neighborhood').sum().reset_index()\ntoronto_grouped.head()\n\nfun_hoods = toronto_grouped.shape[0]\n\nprint(fun_hoods)"
        }, 
        {
            "source": "Before ranking which venue types are found the most for each neighborhood, any neighborhood with fewer than 8 venues will be dropped.  For instance, if we want to cluster neighborhoods by the top 5 most occuring types of venue but there are only 3 venues in a neighborhood, the 4th and 5th most occurring venue type will have zero venues of that type in the target neighborhood.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "for i in range(fun_hoods):\n    if toronto_grouped.agg('sum', axis=1)[i] <8:\n        toronto_grouped.drop([i], inplace=True)\ntoronto_grouped.reset_index(inplace=True, drop=True)\n\ntoronto_grouped.head()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def return_most_venues(row, num_top_venues):\n    row_categories = row.iloc[1:]\n    row_categories_sorted = row_categories.sort_values(ascending=False)\n    \n    return row_categories_sorted.index.values[0:num_top_venues]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "num_top_venues = 5\n\nindicators = ['st', 'nd', 'rd']\n\n# create columns for number of top venues\ncolumns = ['Neighborhood']\nfor ind in np.arange(num_top_venues):\n    try:\n        columns.append('{}{} Most Venue'.format(ind+1, indicators[ind]))\n    except:\n        columns.append('{}th Most Venue'.format(ind+1))\n\nneighborhoods_venues_sorted = pd.DataFrame(columns=columns)\nneighborhoods_venues_sorted['Neighborhood'] = toronto_grouped['Neighborhood']\n\nfor ind in np.arange(toronto_grouped.shape[0]):\n    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_venues(toronto_grouped.iloc[ind, :], num_top_venues)\n\nprint(neighborhoods_venues_sorted.shape)\nneighborhoods_venues_sorted.head()"
        }, 
        {
            "source": "### 4. Clustering of Neighborhoods", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from sklearn.cluster import KMeans\n\n# set number of clusters\nkclusters = 5\n\ntoronto_grouped_clustering = toronto_grouped.drop('Neighborhood', 1)\n\n# run k-means clustering\nkmeans = KMeans(n_clusters=kclusters, random_state=0).fit(toronto_grouped_clustering)\n\n# check cluster labels generated for each of first 10 rows\nkmeans.labels_[0:10]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# add clusters labels to the dataframe\nneighborhoods_venues_sorted.insert(0, 'Cluster Label', kmeans.labels_)\n\ntoronto_merged = toronto_data\n\n# merge toronto_grouped with toronto_data to add latitude/longitude\ntoronto_merged = toronto_merged.join(neighborhoods_venues_sorted.set_index('Neighborhood'), how='right', on='Neighbourhood')\n\ntoronto_merged.head()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "toronto_merged.tail()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "toronto_merged.shape"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "map_clusters = folium.Map(location=[start_lat, start_long], zoom_start=11)\n\n# set color scheme for clusters\nx = np.arange(kclusters)\nys = [i + x + (i*x)**2 for i in range(kclusters)]\ncolors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\nrainbow = [colors.rgb2hex(i) for i in colors_array]\n\n# add markers to the map\nmarkers_colors = []\nfor lat, lon, poi, cluster in zip(toronto_merged['Latitude'], toronto_merged['Longitude'], toronto_merged['Neighbourhood'], toronto_merged['Cluster Label']):\n    label = folium.Popup(str(poi) + 'Cluster' + str(cluster), parse_html=True)\n    folium.CircleMarker(\n    [lat, lon],\n    radius=5,\n    popup=label,\n    color=rainbow[cluster-1],\n    fill=True,\n    fill_color=rainbow[cluster-1],\n    fill_opacity=0.7).add_to(map_clusters)\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "map_clusters"
        }, 
        {
            "source": "Each of the 5 clusters can be evaluated in closer detail to illustrate the differences between them.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "toronto_merged.loc[toronto_merged['Cluster Label'] == 0, toronto_merged.columns[[2] + list(range(5, toronto_merged.shape[1])) ]]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "toronto_merged.loc[toronto_merged['Cluster Label'] == 1, toronto_merged.columns[[2] + list(range(5, toronto_merged.shape[1])) ]]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "toronto_merged.loc[toronto_merged['Cluster Label'] == 2, toronto_merged.columns[[2] + list(range(5, toronto_merged.shape[1])) ]]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "toronto_merged.loc[toronto_merged['Cluster Label'] == 3, toronto_merged.columns[[2] + list(range(5, toronto_merged.shape[1])) ]]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "toronto_merged.loc[toronto_merged['Cluster Label'] == 4, toronto_merged.columns[[2] + list(range(5, toronto_merged.shape[1])) ]]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}